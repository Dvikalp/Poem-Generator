{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Use lyrics from Irish songs then see how the generated text looks like.\n",
        "\n",
        "->Building the Word Vocabulary\n",
        "\n",
        "->Preprocessing the Dataset\n",
        "\n",
        "->Build, compile and train the Model\n",
        "\n",
        "->Generating Text"
      ],
      "metadata": {
        "id": "QA7T3fMrAmHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "77f59F_sO60N"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 15UqmiIm0xwh9mt0IYq2z3jHaauxQSTQT"
      ],
      "metadata": {
        "id": "o_LJWlapSL5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=open('./irish-lyrics-eof.txt').read()\n",
        "corpus=data.lower().split(\"\\n\")\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "py2glcfWSN2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words=len(tokenizer.word_index)+1\n",
        "\n",
        "print(f'word index dictionary: {tokenizer.word_index}')\n",
        "print(f'total words: {total_words}')"
      ],
      "metadata": {
        "id": "m5wIwBK6StXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences=[]\n",
        "for line in corpus:\n",
        "  token_list=tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1,len(token_list)):\n",
        "    n_gram_sequence=token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len=max([len(x) for x in input_sequences])\n",
        "input_sequences=pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre')\n",
        "xs,labels=input_sequences[:,:-1],input_sequences[:,-1]\n",
        "ys=tf.keras.utils.to_categorical(labels,num_classes=total_words)"
      ],
      "metadata": {
        "id": "AnKmZOMKT6Qo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get sample sentence\n",
        "sentence = corpus[0].split()\n",
        "print(f'sample sentence: {sentence}')\n",
        "\n",
        "token_list = []\n",
        "\n",
        "# Look up the indices of each word and append to the list\n",
        "for word in sentence: \n",
        "  token_list.append(tokenizer.word_index[word])\n",
        "\n",
        "print(token_list)"
      ],
      "metadata": {
        "id": "ChMUifJCW5Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elem_number = 5\n",
        "\n",
        "# Print token list and phrase\n",
        "print(f'token list: {xs[elem_number]}')\n",
        "print(f'decoded to text: {tokenizer.sequences_to_texts([xs[elem_number]])}')\n",
        "\n",
        "# Print label\n",
        "print(f'one-hot label: {ys[elem_number]}')\n",
        "print(f'index of label: {np.argmax(ys[elem_number])}')"
      ],
      "metadata": {
        "id": "ttRpb_10aB_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim=128\n",
        "lstm_units=256\n",
        "\n",
        "model=Sequential([\n",
        "    Embedding(total_words,embedding_dim,input_length=max_sequence_len-1),\n",
        "    Bidirectional(LSTM(lstm_units)),\n",
        "    Dense(total_words,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "AVIZKSO3ahxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "\n",
        "history = model.fit(xs, ys, epochs=epochs)"
      ],
      "metadata": {
        "id": "nBug6sA0c1Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define seed text\n",
        "\n",
        "seed_text = input(print(\"Enter something to start your poem\"))\n",
        "\n",
        "\n",
        "# Define total words to predict\n",
        "next_words = 100\n",
        "\n",
        "# Loop until desired length is reached\n",
        "for _ in range(next_words):\n",
        "\n",
        "\t# Convert the seed text to a token sequence\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "\t# Pad the sequence\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\t\n",
        "\t# Feed to the model and get the probabilities for each index\n",
        "\tprobabilities = model.predict(token_list, verbose=0)\n",
        "\n",
        "\t# Get the index with the highest probability\n",
        "\tpredicted = np.argmax(probabilities, axis=-1)[0]\n",
        "\n",
        "\t# Ignore if index is 0 because that is just the padding.\n",
        "\tif predicted != 0:\n",
        "\t\t\n",
        "\t\t# Look up the word associated with the index. \n",
        "\t\toutput_word = tokenizer.index_word[predicted]\n",
        "\n",
        "\t\t# Combine with the seed text\n",
        "\t\tseed_text += \" \" + output_word\n",
        "\n",
        "# Print the result\t\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dolkrm9c1Uz2",
        "outputId": "2bf84730-5883-4ca4-8859-bb240b88159c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter something to start your poem\n",
            "NoneI am Keshav and I like to fuck men\n",
            "I am Keshav and I like to fuck men my barley and fair as the wind and they bore your heart in the again loud and ground more they they did be seen from sea and fathers of wind and bells did be reap those wind open sun ones in art ones spend buy they bubblin wind in art drawn open immortal little open dublins mullingar ones wind and fair old ones wind buy turn the rocky street in the night o they night they take me from the sea young young had best wed gone and young more young fair before fair fair never seen the grey brave\n"
          ]
        }
      ]
    }
  ]
}